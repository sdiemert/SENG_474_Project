\documentclass[11pt, notitlepage,abstracton,oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}

\title{Analysis of Medical Device Failure Reports from the MAUDE Database: A Data Mining Approach}
\author{Simon Diemert, Scott Low, Paul Moon}
\date{April 4th 2015}							% Activate to display a given date or no date

\begin{document}
\maketitle

\begin{abstract}

%REFERENCE FORMAT: http://cs.stanford.edu/people/widom/paper-writing.html#related

\thispagestyle{empty}
The Manufacture User Facility Device Experience (MAUDE) database contains over 3.7 million records of medical device adverse events in the United States dating back to 1991. This database is maintained by the US Food and Drug Administration (FDA). Until recently, this data has only been accessible to the public via manual queries to the MAUDE. In January 2015, the FDA published an open application programming interface (API) that can be programmatically accessed. Each record in the FDA contains metadata about the device failure and a textual description created by the reporter of the adverse event. Two analyses of records from the MAUDE were conducted. The first focused on predicting adverse events from the metadata of each record; the other focused on classifying adverse events using the natural language description of the adverse event. In both cases, several different data mining techniques were trialled, with a Naive Bayes classifier showing the best results. The results for the metadata analysis were varied. On average, the F-Measure for this approach was 0.764. The natural language classification approach had much poorer results, likely due to the unsophisticated approach of word frequency counts used for this analysis.
\end{abstract}

\tableofcontents

\clearpage
\newpage
\setcounter{page}{1}
\section{Introduction}
Modern medicine relies heavily on technology to support the delivery of treatments and care. These technologies are described by the umbrella term ``medical devices" which encompasses any combination of mechanical, electrical, software devices that are used in medical care. The implications of these devices failing can range from minor annoyances to death. Adverse events related to medical devices in the United States are reported to the Food and Drug Administration (FDA). These reports are stored in the \textbf{Ma}nufacturer and \textbf{U}ser Facility \textbf{D}evice \textbf{E}xperience (MAUDE) database and are publicly accessible. The MAUDE database contains approximately 3.7 million records dating back to 1991. 

Previous works have focused on analyzing the data in the MAUDE database to extract trends related to specific devices \cite{weber_preliminary_2011}. However, many of these analyses have been conducted using a manual review process that does not lend itself well to analyzing the large number of records available in the MAUDE database. The FDA has recently made the data accessible via its openFDA API which allows programmatic access to the data. To the best of our knowledge, no work analyzing data obtained from this new API has been published.

Health care is a complex domain that requires medical devices to interact with humans in increasingly complicated ways. There are many examples of adverse medical events that were caused by a combination of a device and human users. Horsky conducted an analysis of such an event \cite{horsky_2005} in which a clinical information system and its users caused a near-fatal overdose of potassium choloride. Clearly, safety analyses of medical devices must consider more than just the device. They must also consider how the device interacts with human actors \cite{karsh_health_2010}. Leveson has presented general a model for analyzing the behaviour of complex sociotechnical systems called the STAMP model. The STAMP model describes a system as a feedback control loop with a controlled process being actuated and sensed by a human actor \cite{leveson_engineering_2012}. Work by Mason-Blakely and Weber has tailored this model for analysis of software in health care. This model is shown in Figure \ref{fig:stamp-emr} below \cite{stamp_emr_2011}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.95\textwidth]{figures/stamp-emr}
	\caption{STAMP - EMR model from \cite{stamp_emr_2011}}
	\label{fig:stamp-emr}
\end{figure}

Mason-Blakey and Habibi used the STAMP EMR model to classify 350 adverse event reports related to software in the MAUDE database (not yet published). This classification was conducted by manually inspecting the natural language summary of each event. A number of different classes (related to the STAMP EMR model) were assigned to each record depending on the reviewer's understanding on the adverse event and how it fits into the STAMP EHR model. The categories used were: 

\begin{itemize}
	\item Care Provider
	\item Point of Care Sensor
	\item Point of Decision Display
	\item Point of Care Actuator
	\item Point of Decision Control
	\item Medical Device Decision Support (MDDS)
	\item Technical Process
\end{itemize}

In this project the MAUDE database was examined algorithmically using standard data mining techniques. Two different approaches were used to analyze data from MAUDE database:  i) analysis of the metadata for each record to predict the outcome of the event (injury, death, etc.); ii) analysis of the natural language description of the adverse event in an attempt to classify the outcome of each record (as outlined in the STAMP EMR model). The remainder of this paper is structured as follows: section 2 describes the methods used to collect and analyze this data; section 3 presents results; and section 4 provides a discussion of results and outlines strengths and weaknesses of the aforementioned approaches. 

\section{Methods}
This section describes two approaches to analyzing the data from the MAUDE. The first approach used the metadata from a large number of events to attempt to predict event outcomes. The second approach used a small set of events (previously classified by Mason-Blakey and Habibi) and attempted to classify these based on the natural language summary of the adverse event.

\subsection{Approach I: Metadata Analysis}
The first approach that was considered for classifying records obtained from the MAUDE was the analysis of important metadata fields. In order to complete this approach, a Python script was written to mine the openFDA API over a specified date range and obtain raw JSON dumps of the data. Once this was complete, the results were manually inspected and a list of important metadata attributes was created. These attributes were as follows:

\begin{itemize}
	\item \textbf{device\_operator:} The person using the medical device at the time of the adverse event.
	\item \textbf{manufacturer\_name:} The name of the manufacturer who created the device in question.
	\item \textbf{reporter\_occpation\_code:} The occupation code of the person who reported the failure of the device in question.
	\item \textbf{device\_report\_product\_code:} Three-letter FDA Product Classification Code. Medical devices are classified under 21 CFR Parts 862-892.
	\item \textbf{expired:} A boolean flag with a value of \textit{true} if the device was past its manufacturer-provided expiry date at the time its failure was reported.
\end{itemize}

Once this list of metadata attributes was obtained, a second Python script was created to merge and sanitize the collected data, filtering out metadata attributes that were deemed unimportant before creating a .arff file for data analysis in Weka.

\subsubsection{Data Collection}
As previously described, the first step of the metadata analysis approach was to write a Python script that was able to scrape the openFDA API for all records within a specified date range. After obtaining an API key from openFDA, the script was written such that it grabbed records starting from January 1, 2014 in 100-record chunks as this is the maximum number of records that was allowed to be returned in a single API call. 

It is worth noting that for the sake of virtual memory management, the script would write 10,000 records to a single text file before closing that file and opening a new one for the next 10,000 records. This also helped to ensure that as much data as possible would be preserved in the event of a script failure (due to, for example, loss of internet connection during execution). 

Once the script was written and run for the first time, it was noted that a large number of HTTP 403 errors were being returned from the openFDA API. After consulting the API documentation, it was determined that this was due to the script exceeding the maximum number of API requests allowed per second. As a result, a timeout of half a second had to be added to each outgoing request to ensure that the number of requests per second remained below this limit.

For the scope of this experiment, the first 50,000 of the 602,117 medical device failure records from the year 2014 were used in this approach. It is important to note, however, that the previously described script, is able to download any number of records between any range of dates. In the future, it would be interesting to run the script for longer to observe the effects that a larger dataset has on the results of the classification process.

\subsubsection{Data Pre-Processing}
The data pre-processing phase of the first approach was by far the most time consuming phase due to a number of challenges that were experienced when attempting to merge and parse the 10,000-record JSON dumps that were obtained in the data collection step into a single .arff file. To process the data, a second Python script was run that would open each of the 10,000-record JSON dumps, extract the important metadata attributes (as described in section 2.1), and write these to a .arff file. Non-unicode characters in the JSON dumps, however, meant that any records containing such characters could not be written to the .arff and needed to be skipped. As a result, ASCII encoding of each metadata attribute for each record needed to be attempted before writing it to the .arff if and only if all encoding attempts for the specified record succeeded.

Once the .arff was created and loaded into Weka, it became apparent that due to the nominal nature of the metadata attributes, all possible values for each of the metadata attributes needed to be included in each one's respective @ATTRIBUTE definition at the beginning of the .arff. As a result, code to extract all possible values for each metadata attribute (while removing duplicates and skipping records with null values)	 needed to be written. 

Upon rerunning the script with the aforementioned logic in place, a .arff file with fully defined, nominal attributes was created and could be loaded into Weka for analysis. 

\subsubsection{Data Analysis}
The data analysis phase for this approach was relatively straightforward. The .arff generated in the previous step was loaded into Weka and a Naive Bayes classifier with 10 fold cross-validation was used to analyze the data. After running the classifier, it was observed that 76.67\% of all records were classified correctly.

It is worth mentioning that a number of other classifiers, such as J48, Bayesian Network, and ID3 were also run simply to observe results, but due to the large amount of data being processed, none of these produced meaningful results. 

\subsection{Approach II: Record Categorization}
The second approach for classifying records from the MAUDE was to use the report description text in an attempt to classify records based on the STAMP - EMR model described earlier. Report descriptions for each record in the MAUDE were found in their respective mdr\_text metadata fields. A sample mdr\_text record has been provided for reference below:

\begin{quote}
``IT WAS REPORTED THAT THE CUSTOMER'S INSULIN PUMP HAD AN ERROR ALARM WHILE CHANGING THE RESERVOIR AND INSULIN WAS SQUIRTING OUT. ADVISED THAT THE INSULIN PUMP WILL BE REPLACED. NO FURTHER INFORMATION WAS PROVIDED"
\end{quote} 

\subsubsection{Data Collection}
Mason-Blakey and Habibi's initial 353 classifications, which were classified in this approach using 10 fold cross-validation, were provided as a CSV file. The file contained identification numbers for each record, as well as classifications for each event, where the classifications were one of the following in Table \ref{tab:OriginalClassifications}. The file denoted 0 or 1 depending on how many reporters classified that event a specific class.

The medical device failure records used in this approach were pulled from the openFDA API in a method similar to that described in section 2.1.1 above. Rather than pulling down the 50,000 most recent records, however, the identification numbers from the provided CSV file were used to query the database for the 353 records corresponding to Mason-Blakey and Habibi's initial classifications. The next step in this approach was to sanitize the input data for processing in Weka.

\subsubsection{Data Pre-Processing}
Data pre-processing required nontrivial data cleaning and natural language processing steps. First, a Python script named nlp.py extracted the mdr\_text from the MAUDE records obtained in the previous step. The script joined all mdr\_text records for a single event into a single string, as one event could have many event report descriptions. Using a Python library named nltk (Natural Language ToolKit), the words were tokenized and tagged according to their part-of-speech. For example, "failure" was tagged as NN (noun), and "lifescan" was tagged as JJ (adjective). The frequencies of tokens were also counted using Python's collections counter data structure. Furthermore, if a token did not satisfy any of the three following conditions, it was discarded.

\begin{itemize}
	\item \textbf{Stopword}: If the token was a stopword such as "the", "is", "which" etc., it was discarded because we considered stopwords as semantically insignificant.
	\item \textbf{Minimum word length}: If the word was less than or equal to 4 characters, the word was discarded because there were many semantically insignificant words such as "ohms", "10x", "hour" etc. that were short in word length. We discovered there were more interesting words with word length $\geq 4$ such as "tachycardia", "investigation", and "incision".
	\item \textbf{Part-of-speech}: Only tokens tagged as "NN" (noun) were considered as important for this classification approach, because the classification method was only counting for the frequency of words, and it did not use advanced data mining tools that utilized part-of-speech.
\end{itemize}

Furthermore, we decided that there were too many classes in Mason-Blakey and Habibi's classifications. As a result, the classes within the CSV file was simplified by combining similar classes, such as "PointOfCare.Sensor.Human" and "PointOfCare.Sensor.Machine" into "PointOfCareSensor". The simplified classes can be seen in Table \ref{tab:SimplifiedClassifications} in the appendix of this report.

To generate the .arff file necessary for data mining with Weka, nlp.py kept a dictionary of all valid words and wrote "@data [word] NUMERIC" for each word in the dictionary. The script kept track of the dictionary index for each word, and for each event, the script generated a string which encoded the count of words with the count in its appropriate index. For example, if the word "tachycardia" was the fifth word in the dictionary and an event's mdr\_text contained three instances of "tachycardia", then the string may look like "0, 0, 0, 0, 3, ..., 0".

An .arff file was generated for each of the nine classes to accommodate the fact that an event could be classified by more than one class. Each .arff consisted of 2010 unique words and 256 instances.

\subsubsection{Data Analysis}
The generated .arff files were loaded into Weka, and the Naive Bayes classifier with 10-fold cross-validation was employed. Other classifiers such as BayesNet (Bayesian Network) were explored, but the Naive Bayes classifier fit the data set most appropriately because text categorization using frequency of words is often solved with Naive Bayes models, as taught in class.

\section{Results}
The following section of the report outlines the results obtained by both approaches explored in this project.

\subsection{Approach I: Metadata Analysis}
The results of running a 10-fold cross validation Naive Bayes classifier on the 50,000 records obtained as described in section 2.1 are show below.

\begin{table}[htdp]
\caption{Detailed Accuracy by Class}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
& Precision & Recall & F-Measure \\ \hline
Class = Death & 0.522 & 0.184 & 0.276 \\ \hline
Class = Injury & 0.701 & 0.783 & 0.740 \\ \hline
Class = Malfunction & 0.844 & 0.806 & 0.825 \\ \hline
Class = Other & 0.310 & 0.326 & 0.318 \\ \hline
Class = No answer provided & 0.515 & 0.449 & 0.480 \\ \hline
Weighted Average & 0.768 & 0.767 & 0.764 \\ \hline
\end{tabular}
\end{center}
\label{default}
\end{table}

Furthermore, the ROC curves obtained for each of the five possible classes of records, as well as the area under each curve can be found in Figure \ref{fig:ROC Curves} below. These were used to compare the Naive Bayes classifier to other classifiers in Weka such as Bayes Net. Although these ROC curves have not been shown in this report, it is worth noting that the area under the curves generated by the Naive Bayes classifier were on average higher than those produced by all other classifiers examined for this approach.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/death}
        \caption{Death. Area: 0.8412}
        \label{fig:death}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/injury}
        \caption{Injury. Area: 0.8843}
        \label{fig:injury}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/other}
        \caption{Other. Area: 0.9139}
        \label{fig:other}
    \end{subfigure}    
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/malfunction}
        \caption{Malfunction. Area: 0.8869}
        \label{fig:injury}
    \end{subfigure} 
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/no-answer}
        \caption{No Answer. Area: 0.9084}
        \label{fig:no-figure}
    \end{subfigure}          
    \caption{ROC Curves for Metadata Classification}
    \label{fig:ROC Curves}
\end{figure}

\subsection{Approach II: Report Description Categorization}

The results of running the 10-fold Naive Bayes classifier on each of the 9 classes are shown below.

\begin{table}[H]
    \caption{Naive Bayes Classifier on CareProviderOutput Class}
        \begin{center}
        \begin{tabular}{|c|c|c|c|}
            \hline
            & Precision & Recall & F-Measure \\ \hline
            Class = Present & 0.955 & 0.892 & 0.922 \\ \hline
            Class = Absent & 0.188 & 0.375 & 0.25 \\ \hline
            Weighted Average & 0.907 & 0.859 & 0.88 \\ \hline
        \end{tabular}
        \end{center}
    \label{default}
\end{table}%

\begin{table}[H]
    \caption{Naive Bayes Classifier on EMROutput Class}
        \begin{center}
        \begin{tabular}{|c|c|c|c|}
            \hline
            & Precision & Recall & F-Measure \\ \hline
            Class = Present & 0.189 & 0.313 & 0.235 \\ \hline
            Class = Absent & 0.892 & 0.808 & 0.848 \\ \hline
            Weighted Average & 0.804 & 0.746 & 0.771\\ \hline
        \end{tabular}
        \end{center}
    \label{default}
\end{table}%

\begin{table}[H]
    \caption{Naive Bayes Classifier on HealthCareProcessOutput Class}
        \begin{center}
        \begin{tabular}{|c|c|c|c|}
            \hline
            & Precision & Recall & F-Measure \\ \hline
            Class = Present & 0.864 & 0.868 & 0.866 \\ \hline
            Class = Absent & 0.171 & 0.167 & 0.169 \\ \hline
            Weighted Average & 0.767 & 0.77 & 0.768 \\ \hline
        \end{tabular}
        \end{center}
    \label{default}
\end{table}%

\begin{table}[H]
    \caption{Naive Bayes Classifier on MDDSOutput Class}
        \begin{center}
        \begin{tabular}{|c|c|c|c|}
            \hline
            & Precision & Recall & F-Measure \\ \hline
            Class = Present & 0.907 & 0.833 & 0.869 \\ \hline
            Class = Absent & 0.049 & 0.091 & 0.063 \\ \hline
            Weighted Average & 0.833 & 0.77 & 0.799 \\ \hline
        \end{tabular}
        \end{center}
    \label{default}
\end{table}%

\begin{table}[H]
    \caption{Naive Bayes Classifier on PointOfCareActuatorOutput Class}
        \begin{center}
        \begin{tabular}{|c|c|c|c|}
            \hline
            & Precision & Recall & F-Measure \\ \hline
            Class = Present & 0.661 & 0.73 & 0.694 \\ \hline
            Class = Absent & 0.266 & 0.207 & 0.233 \\ \hline
            Weighted Average & 0.535 & 0.563 & 0.546 \\ \hline
        \end{tabular}
        \end{center}
    \label{default}
\end{table}%

\begin{table}[H]
    \caption{Naive Bayes Classifier on PointOfCareSensorOutput Class}
        \begin{center}
        \begin{tabular}{|c|c|c|c|}
            \hline
            & Precision & Recall & F-Measure \\ \hline
            Class = Present & 0.793 & 0.647 & 0.712 \\ \hline
            Class = Absent & 0.228 & 0.382 & 0.286 \\ \hline
            Weighted Average & 0.671 & 0.59 & 0.621 \\ \hline
        \end{tabular}
        \end{center}
    \label{default}
\end{table}%

\begin{table}[H]
    \caption{Naive Bayes Classifier on PointOfDecisionControlOutput Class}
        \begin{center}
        \begin{tabular}{|c|c|c|c|}
            \hline
            & Precision & Recall & F-Measure \\ \hline
            Class = Present & 0.804 & 0.792 & 0.798 \\ \hline
            Class = Absent & 0.173 & 0.184 & 0.178 \\ \hline
            Weighted Average & 0.683 & 0.676 & 0.679 \\ \hline
        \end{tabular}
        \end{center}
    \label{default}
\end{table}%

\begin{table}[H]
    \caption{Naive Bayes Classifier on PointOfDecisionDisplayOutput Class}
        \begin{center}
        \begin{tabular}{|c|c|c|c|}
            \hline
            & Precision & Recall & F-Measure \\ \hline
            Class = Present & 0.732 & 0.755 & 0.743 \\ \hline
            Class = Absent & 0.258 & 0.235 & 0.246 \\ \hline
            Weighted Average & 0.606 & 0.617 & 0.611 \\ \hline
        \end{tabular}
        \end{center}
    \label{default}
\end{table}%

\begin{table}[H]
    \caption{Naive Bayes Classifier on TechnicalProcessOutput Class}
        \begin{center}
        \begin{tabular}{|c|c|c|c|}
            \hline
            & Precision & Recall & F-Measure \\ \hline
            Class = Present & 0.869 & 0.869 & 0.869 \\ \hline
            Class = Absent & 0.349 & 0.349 & 0.349 \\ \hline
            Weighted Average & 0.781 & 0.781 & 0.781 \\ \hline
        \end{tabular}
        \end{center}
    \label{default}
\end{table}%

The ROC curves for each of the classes are shown in Figure \ref{fig:Approach 2 ROC Curves} below.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/approach2/CareProvider-False(0)}
        \caption{CareProvider-False. Area: 0.5828}
        \label{fig:CareProvider-False(0)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/approach2/CareProvider-True(1)}
        \caption{CareProvider-True. Area: 0.5589}
        \label{fig:CareProvider-True(1)}
    \end{subfigure}
    \hfill

    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/approach2/EMR-False(0)}
        \caption{EMR-False. Area: 0.5344}
        \label{fig:EMR-False(0)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/approach2/EMR-True(1)}
        \caption{EMR-True. Area: 0.539}
        \label{fig:EMR-True(1)}
    \end{subfigure}
    \hfill

    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/approach2/HealthCareProcess-False(0)}
        \caption{HealthCareProcess-False. Area: 0.576}
        \label{fig:HealthCareProcess-False(0)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/approach2/HealthCareProcess-True(1)}
        \caption{HealthCareProcess-True. Area: 0.585}
        \label{fig:HealthCareProcess-True(1)}
    \end{subfigure}
    \hfill

    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/approach2/MDDS-False(0)}
        \caption{MDDS-False. Area: 0.4954}
        \label{fig:MDDS-False(0)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/approach2/MDDS-True(1)}
        \caption{MDDS-True. Area: 0.4672}
        \label{fig:MDDS-True(1)}
    \end{subfigure}
    \hfill
\end{figure}

\begin{figure}[H]
    \ContinuedFloat
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/approach2/PointOfCareActuator-False(0)}
        \caption{PointOfCareActuator-False. Area: 0.4586}
        \label{fig:PointOfCareActuator-False(0)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/approach2/PointOfCareActuator-True(1)}
        \caption{PointOfCareActuator-True. Area: 0.4585}
        \label{fig:PointOfCareActuator-True(1)}
    \end{subfigure}
    \hfill

    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/approach2/PointOfCareSensor-False(0)}
        \caption{PointOfCareSensor-False. Area: 0.5395}
        \label{fig:PointOfCareSensor-False(0)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/approach2/PointOfCareSensor-True(1)}
        \caption{PointOfCareSensor-True. Area: 0.533}
        \label{fig:PointOfCareSensor-True(1)}
    \end{subfigure}
    \hfill

    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/approach2/PointOfDecisionControl-False(0)}
        \caption{PointOfDecisionControl-False. Area: 0.4983}
        \label{fig:PointOfDecisionControl-False(0)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/approach2/PointOfDecisionControl-True(1)}
        \caption{PointOfDecisionControl-True. Area: 0.5061}
        \label{fig:PointOfDecisionControl-True(1)}
    \end{subfigure}
    \hfill

    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/approach2/PointOfDecisionDisplay-False(0)}
        \caption{PointOfDecisionDisplay-False. Area: 0.5179}
        \label{fig:PointOfDecisionDisplay-False(0)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/approach2/PointOfDecisionDisplay-True(1)}
        \caption{PointOfDecisionDisplay-True. Area: 0.5195}
        \label{fig:PointOfDecisionDisplay-True(1)}
    \end{subfigure}
    \hfill
\end{figure}

\begin{figure}[H]
    \ContinuedFloat

    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/approach2/TechnicalProcess-False(0)}
        \caption{TechnicalProcess-False. Area: 0.6259}
        \label{fig:TechnicalProcess-False(0)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/approach2/TechnicalProcess-True(1)}
        \caption{TechnicalProcess-True. Area: 0.6386}
        \label{fig:TechnicalProcess-True(1)}
    \end{subfigure}
    \hfill

    \caption{ROC Curves for mdr\_text Classification}
    \label{fig:Approach 2 ROC Curves}
\end{figure}


\section{Discussion}

For the metadata analysis approach, it was observed that class = Malfunction was extremely well classified, with precision of 0.844, recall of 0.806 and f-measure of 0.825. Despite this, however, poor results (such as those observed for class = Death and class = Other) were also observed. Overall, the average f-measure was 0.764, which indicates that there is some correlation between the five selected metadata attributes outlined in section 2.1 and the possible classifier outcomes. The lower f-measure results observed are likely due to either noise in the data or to the fact that some metadata attributes with several potential values (such as manufacturer\_name) are not accurate predictors of classifications. In an attempt to mediate the latter of these two issues, certain metadata attributes were removed from the dataset to see if results would change significantly. No major changes, however, were observed.

Furthermore, the analysis of device\_report\_product\_code exposed several interesting, though perhaps unsurprising trends in data. For example, DTB, one of the device\_report\_product\_codes with the highest number of associated deaths (88 occurrences) was looked up in the FDA's product classification database and was observed to be a "Permanent Pacemaker Electrode", a relatively high-risk device. Another example of a product code that had a relatively high number of deaths (92) associated with it was NIQ, or a coronary drug-eluting stent.

Further improvements that could be made to the metadata analysis approach would be to manually remove noise from data (such as records with classifications of Other or No Answer Provided), or to consider other combinations of metadata attributes for analysis. Since the number of useful metadata attributes available from the openFDA API was observed to be relatively low, however, we believe that making improvements to the second approach outlined in this report would produce more meaningful results.

For the record description categorization approach, the Naive Bayes classifier showed promising f-measures for many of the classes. The best f-measure belonged to the CareProviderOutput (class = present) class with a value of 0.922. However, seven out of nine classes showed f-measures below 0.25 for at least one of their true/false classes, suggesting room for improvement. An interesting observation is that for all classes, the f-measures for one true/false class was significantly higher than the other e.g. f-measure of PointOfDecisionControlOutput for class = present was 0.798 compared to the f-measure of class = absent at 0.178.

The ROC curves in Figure \ref{fig:Approach 2 ROC Curves} show that the Naive Bayes classifier's performance was subpar. PointOfCareActuator-True ROC curve showed an area of 0.4585, which is marginally better than a classifier which predicts by flipping a coin. The best area under ROC belonged to TechnicalProcess (True) with an area of 0.6386.

These results suggest that using word frequencies is still a naive method of implementing this classification approach. Future work includes extensive exploration with various natural language processing techniques such as analyzing part-of-speech and n-grams; sentimental analysis; and increasing the sample size from 256 to a larger number.

\section{Conclusion}
Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.

\section{Appendix}

\begin{table}[htdp]
\caption{Original Classifications}
\label{tab:OriginalClassifications}
\begin{center}
\begin{tabular}{|p{80mm}|}
\hline
CareProvider.Integrity.Data \newline CareProvider.Integrity.Relation \newline CareProvider.Integrity.Knowledge \\ \hline
PointOfDecision.Display.Human \newline PointOfDecision.Display.Machine \\ \hline
PointOfDecision.Control.Human \newline PointOfDecision.Control.Machine \\ \hline
PointOfCare.Sensor.Human \newline PointOfCare.Sensor.Machine \\ \hline
PointOfCare.Actuator.Human \newline PointOfCare.Actuator.Machine \\ \hline
MDDS \\ \hline
EMR.Usability.Operability \newline EMR.Usability.Knowability \\ \hline
EMR.Availibility.Reliability \newline EMR.Availibility.FaultTolerence \\ \hline
EMR.Integrity.Physical \newline EMR.Integrity.Data \newline EMR.Integrity.Relation \newline EMR.Integrity.Knowledge \\ \hline
HealthCareProcess.Controller.Capital \newline HealthCareProcess.Controller.Responsiveness \newline HealthCareProcess.Controller.ClinicalIntegrity \\ \hline
HealthCareProcess.Sensor \newline HealthCareProcess.Display \newline HealthCareProcess.Actuator \newline HealthCareProcess.Control \\ \hline
TechnicalProcess.Controller.Capital \newline TechnicalProcess.Controller.Responsiveness \newline TechnicalProcess.Controller.TechnicalIntegrity \newline TechnicalProcess.Sensor \newline TechnicalProcess.Actuator \\ \hline
\end{tabular}
\end{center}
\label{default}
\end{table}%

\begin{table}[htdp]
\caption{Simplified Classifications}
\label{tab:SimplifiedClassifications}
\begin{center}
\begin{tabular}{|p{40mm}|}
\hline
CareProvider \\ \hline
PointOfDecisionDisplay \\ \hline
PointOfDecisionControl \\ \hline
PointOfCareSensor \\ \hline
PointOfCareActuator \\ \hline
MDDS \\ \hline
EMR \\ \hline
HealthCareProcess \\ \hline
TechnicalProcess \\ \hline
\end{tabular}
\end{center}
\label{default}
\end{table}%

\clearpage

\bibliographystyle{plain}
\bibliography{report}


\end{document}  
